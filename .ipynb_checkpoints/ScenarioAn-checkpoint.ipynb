{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis as maha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- Load Data\n",
    "- Calculate yield curve slope and Y/Y changes in relevant columns\n",
    "\n",
    "\n",
    "#### Data Definitions\n",
    "- TNX: US 10y Treasury\n",
    "- US_Corp: ML US Corporate Bond Total Return Index\n",
    "- LIBOR: 3m LIBOR Rate\n",
    "- BAA: Moody's long-term corporate bond yields index\n",
    "- UNRATE: US seasonally-adjusted unemployment rate\n",
    "- SPY: S\\&P 500 Index\n",
    "- IRX: US 3m treasury rate\n",
    "- RGDP: US seasonally-adjusted Real GDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Commodity Index Data\n",
    "data = pd.read_csv('data/data.csv', index_col=0)\n",
    "data.index = pd.to_datetime(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Interpolation Forward fill - for GDP data which is quarterly\n",
    "#Could potentially want to only use quarterly data - set lin_interp = False\n",
    "lin_interp = True\n",
    "\n",
    "if lin_interp:\n",
    "    data['RGDP'] = data['RGDP'].interpolate()\n",
    "else:\n",
    "    data = data.dropna(subset = ['RGDP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yield Curve Slope: 10y yields - 3m yields\n",
    "data['YC_Slope'] =  pd.eval('data.TNX - data.IRX')\n",
    "#Credit Spread: long-term BAA (corp bonds) - 10y treasury rate\n",
    "data['Cred_Spread'] = pd.eval('data.BAA - data.TNX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in Y/Y changes\n",
    "YY_cols = ['CPI', 'RGDP']\n",
    "data[[x + '_Growth' for x in YY_cols]] = data[YY_cols]/data[YY_cols].shift(12 if lin_interp else 4) - 1\n",
    "\n",
    "#Drop null rows\n",
    "data = data.dropna()\n",
    "\n",
    "#Subtract mean\n",
    "# data = data - data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TNX                4.125500\n",
       "US_Corp         1541.163409\n",
       "LIBOR              2.398775\n",
       "CPI              205.650000\n",
       "BAA                6.585000\n",
       "UNRATE             5.400000\n",
       "SPY              125.500000\n",
       "IRX                1.868500\n",
       "RGDP           15180.037833\n",
       "YC_Slope           1.746500\n",
       "Cred_Spread        2.338500\n",
       "CPI_Growth         0.021842\n",
       "RGDP_Growth        0.025657\n",
       "dtype: float64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TNX               1.652370\n",
       "US_Corp         738.421893\n",
       "LIBOR             2.200002\n",
       "CPI              32.089959\n",
       "BAA               1.404468\n",
       "UNRATE            1.631899\n",
       "SPY              62.351690\n",
       "IRX               2.059814\n",
       "RGDP           2547.039609\n",
       "YC_Slope          1.080416\n",
       "Cred_Spread       0.778178\n",
       "CPI_Growth        0.005264\n",
       "RGDP_Growth       0.016129\n",
       "dtype: float64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.856824259398638, 4.9444829301438515)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quick function to get num_sd standard deviations away from the median.\n",
    "#Up pos determines if up is good or bad. Idea being that good scenario is at index 1, bad scenario at index 0.\n",
    "def get_range(data, variable, num_sd, up_pos = True):\n",
    "    i = 1 if up_pos else -1\n",
    "    \n",
    "    v = data[variable]\n",
    "    v = v[(v < v.quantile(.75) ) & (v > v.quantile(.25))]\n",
    "    m = v.mean()\n",
    "    s = v.std()\n",
    "    \n",
    "    return tuple((m - i * num_sd * s, m + i * num_sd * s))\n",
    "\n",
    "get_range(data, 'UNRATE', 1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Scenarios\n",
    "\n",
    "- Define a scenario with a boolean string. Make sure to use spaces between operators (for calculation of mahalanobis distances.\n",
    "- Find empirical mean and covariance matrix of the factors in this scenario.\n",
    "- Encode scenario as a vector, then find mahalanobis distance using scenario vector and empirical mean, covariance.\n",
    "- We then convert scenario Mahalanobis distance into likelihood measure:\n",
    "$$ e^{\\frac{-d}{2}}$$\n",
    "- Rescale probabilities to sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scenario_vars(scenario):\n",
    "    #Get the variables in the scenario. Sort alphabetically for consistent replication\n",
    "    return sorted(list(set([v.split(' ')[0] for v in scenario.split(' & ')])))\n",
    "\n",
    "def get_scenario_vector(scenario):\n",
    "    #Get a vector from a scenario.\n",
    "    vector = {}\n",
    "    for v in scenario.split(' & '):\n",
    "        x = v.split(' ')\n",
    "        cn = x[0]\n",
    "        val = x[2]\n",
    "        vector[cn] = float(val)\n",
    "    vector = pd.DataFrame(vector, index = [0])\n",
    "    colnames = get_scenario_vars(scenario)\n",
    "    return vector[colnames].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGDP_Growth 0.02153049417710918 0.03118893941453791\n",
      "UNRATE 5.856824259398638 4.9444829301438515\n",
      "CPI_Growth 0.020055987437858774 0.023444747654114598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Weak': 'RGDP_Growth <= 0.02153049417710918 & UNRATE >= 5.856824259398638 & CPI_Growth <= 0.020055987437858774',\n",
       " 'Strong': 'RGDP_Growth >= 0.03118893941453791 & UNRATE <= 4.9444829301438515 & CPI_Growth >= 0.023444747654114598',\n",
       " 'Normal': 'RGDP_Growth >= 0.02153049417710918 & RGDP_Growth <= 0.03118893941453791 & UNRATE <= 5.856824259398638 & UNRATE >= 4.9444829301438515 & CPI_Growth >= 0.020055987437858774 & CPI_Growth <= 0.023444747654114598'}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build the scenarios: Weak is >= 1 sd below median, strong is >= 1 sd above median, normal is in range [m - sd, m + sd].\n",
    "\n",
    "scen_names = ['Weak', 'Strong', 'Normal']\n",
    "# relevant_vars = ['RGDP_Growth', 'UNRATE', 'CPI_Growth', 'YC_Slope', 'Cred_Spread', 'TNX']\n",
    "relevant_vars = ['RGDP_Growth', 'UNRATE', 'CPI_Growth']\n",
    "# relevant_vars = ['RGDP_Growth', 'UNRATE']\n",
    "\n",
    "scenarios = {sn: '' for sn in scen_names}\n",
    "\n",
    "up_pos = {v:True for v in relevant_vars}\n",
    "up_pos['UNRATE'] = False\n",
    "up_pos['YC_Slope'] = False\n",
    "up_pos['Cred_Spread'] = False\n",
    "\n",
    "first_run = True\n",
    "\n",
    "for var in relevant_vars:\n",
    "    if first_run:\n",
    "        first_run = False\n",
    "    else:\n",
    "        for v in scen_names:\n",
    "            scenarios[v] += ' & '\n",
    "    \n",
    "    low_bound, u_bound = get_range(data, var, 1, up_pos[var])\n",
    "    \n",
    "    l_sign = '<=' if up_pos[var] else '>='\n",
    "    u_sign = '>=' if up_pos[var] else '<='\n",
    "    \n",
    "    scenarios['Weak'] += ('{} {} {}'.format(var, l_sign, low_bound))\n",
    "    scenarios['Normal'] += ('{} {} {} & {} {} {}'.format(var, u_sign, low_bound, var, l_sign, u_bound))\n",
    "    scenarios['Strong'] += ('{} {} {}'.format(var, u_sign, u_bound))\n",
    "    \n",
    "    print(var, low_bound, u_bound)\n",
    "    \n",
    "scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.65303715e-02 4.93656375e-03 8.28837209e+00]\n",
      "[0.0247467  0.04394743 4.3       ]\n",
      "[0.02199571 0.02214366 5.8       ]\n",
      "{'Weak': 0.2816571428182477, 'Strong': 0.4785803154302323, 'Normal': 0.9819058598498099}\n",
      "{'Weak': 0.1616727739286699, 'Strong': 0.27470777545021197, 'Normal': 0.5636194506211181}\n"
     ]
    }
   ],
   "source": [
    "likelihoods = {}\n",
    "l_sum = 0.0\n",
    "\n",
    "for scenario_name, scenario in scenarios.items():\n",
    "    #Get relevant variables for scenario defined above\n",
    "    scenario_vars = get_scenario_vars(scenario)\n",
    "\n",
    "    #Get the empirical mean & correlation matrix of scenario vars\n",
    "    v = data[scenario_vars].mean().values\n",
    "    scen_corr = data[scenario_vars].corr()\n",
    "\n",
    "    #Encode the scenario as a vector - take empirical averages after conditioning on scenario\n",
    "    u = data.query(scenario)[scenario_vars].mean().values\n",
    "    print(u)\n",
    "    #u = get_scenario_vector(scenario)\n",
    "\n",
    "    #Mahalanobis distance, converted to likelihood\n",
    "    l = np.exp(-maha(u, v, scen_corr)/2)\n",
    "    likelihoods[scenario_name] = l\n",
    "    l_sum += l\n",
    "\n",
    "probs = {sn: l/l_sum for sn, l in likelihoods.items()}\n",
    "print(likelihoods)\n",
    "print(probs)\n",
    "# print(mahala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario = 'RGDP_Growth = 0.01 & UNRATE = 6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disregard below - not right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "1993-01-01    0.007402\n",
       "1993-04-01    0.008183\n",
       "1993-07-01    0.009043\n",
       "1993-10-01    0.009513\n",
       "1994-01-01    0.010511\n",
       "1994-04-01    0.011619\n",
       "1994-07-01    0.013492\n",
       "1994-10-01    0.015674\n",
       "1995-01-01    0.014360\n",
       "1995-04-01    0.015669\n",
       "1995-07-01    0.015091\n",
       "1995-10-01    0.013657\n",
       "1996-01-01    0.014356\n",
       "1996-04-01    0.014343\n",
       "1996-07-01    0.013654\n",
       "1996-10-01    0.011751\n",
       "1997-01-01    0.012357\n",
       "1997-04-01    0.011172\n",
       "1997-07-01    0.010112\n",
       "1997-10-01    0.009153\n",
       "1998-01-01    0.008706\n",
       "1998-04-01    0.007493\n",
       "1998-07-01    0.008279\n",
       "1998-10-01    0.008277\n",
       "1999-01-01    0.007493\n",
       "1999-04-01    0.007494\n",
       "1999-07-01    0.007491\n",
       "1999-10-01    0.006776\n",
       "2000-01-01    0.006453\n",
       "2000-04-01    0.005832\n",
       "                ...   \n",
       "2012-04-01    0.004721\n",
       "2012-07-01    0.004720\n",
       "2012-10-01    0.005764\n",
       "2013-01-01    0.005219\n",
       "2013-04-01    0.006371\n",
       "2013-07-01    0.007406\n",
       "2013-10-01    0.007785\n",
       "2014-01-01    0.010500\n",
       "2014-04-01    0.012841\n",
       "2014-07-01    0.012840\n",
       "2014-10-01    0.015094\n",
       "2015-01-01    0.015092\n",
       "2015-04-01    0.012990\n",
       "2015-07-01    0.011758\n",
       "2015-10-01    0.010641\n",
       "2016-01-01    0.010118\n",
       "2016-04-01    0.010638\n",
       "2016-07-01    0.009625\n",
       "2016-10-01    0.010118\n",
       "2017-01-01    0.009155\n",
       "2017-04-01    0.007880\n",
       "2017-07-01    0.007494\n",
       "2017-10-01    0.006781\n",
       "2018-01-01    0.006782\n",
       "2018-04-01    0.006135\n",
       "2018-07-01    0.006136\n",
       "2018-10-01    0.005839\n",
       "2019-01-01    0.006450\n",
       "2019-04-01    0.005282\n",
       "2019-07-01    0.005553\n",
       "Name: Prob_1, Length: 107, dtype: float64"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define scenario, use spaces between operators for ease of parsing.\n",
    "scenario = 'RGDP_Growth = 0.01 & UNRATE = 6'\n",
    "\n",
    "#Get relevant variables from the scenario defined above.\n",
    "scenario_vars = [v.split(' ')[0] for v in scenario.split(' & ')]\n",
    "\n",
    "#Filter data on the above scenario - not sure if useful or not...\n",
    "scen_data = data.query(scenario)[scenario_vars]\n",
    "\n",
    "#Get the empirical mean & correlation matrix of scenario vars\n",
    "v = data[scenario_vars].mean().values\n",
    "scen_corr = data[scenario_vars].corr()\n",
    "\n",
    "#Calculate mahalanobis distance, transform to likelihood measure\n",
    "data['L_1'] = data[scenario_vars].apply(lambda x: np.exp(-maha(x ,v, scen_corr)/2), raw = True, axis = 1)\n",
    "\n",
    "#Rescale for likelihood\n",
    "data['Prob_1'] = data['L_1'].dropna()/data['L_1'].sum()\n",
    "# data['Prob_1'] = data['Prob_1'].fillna(0)\n",
    "data.Prob_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_1 = 'RGDP_Growth = 0.01 & UNRATE = 6'\n",
    "scenario_2 = 'RGDP_Growth '\n",
    "\n",
    "# l_1 = np.exp(-maha(np.array([float(v.split(' ')[2]) for v in scenario_1.split(' & ')]), v, scen_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RGDP_Growth', 'UNRATE']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scenario_vars(scenario_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
