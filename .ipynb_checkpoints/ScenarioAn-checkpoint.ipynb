{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis as maha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- Load Data\n",
    "- Calculate yield curve slope and Y/Y changes in relevant columns\n",
    "\n",
    "\n",
    "#### Data Definitions\n",
    "- TNX: US 10y Treasury\n",
    "- US_Corp: ML US Corporate Bond Total Return Index\n",
    "- LIBOR: 3m LIBOR Rate\n",
    "- BAA: Moody's long-term corporate bond yields index\n",
    "- UNRATE: US seasonally-adjusted unemployment rate\n",
    "- SPY: S\\&P 500 Index\n",
    "- IRX: US 3m treasury rate\n",
    "- RGDP: US seasonally-adjusted Real GDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Commodity Index Data\n",
    "data = pd.read_csv('data/data.csv', index_col=0)\n",
    "data.index = pd.to_datetime(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Interpolation Forward fill - for GDP data which is quarterly\n",
    "#Could potentially want to only use quarterly data - set lin_interp = False\n",
    "lin_interp = True\n",
    "\n",
    "if lin_interp:\n",
    "    data['RGDP'] = data['RGDP'].interpolate()\n",
    "else:\n",
    "    data = data.dropna(subset = ['RGDP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yield Curve Slope: 10y yields - 3m yields\n",
    "data['YC_Slope'] =  pd.eval('data.TNX - data.IRX')\n",
    "#Credit Spread: long-term BAA (corp bonds) - 10y treasury rate\n",
    "data['Cred_Spread'] = pd.eval('data.BAA - data.TNX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in Y/Y changes\n",
    "YY_cols = ['CPI', 'RGDP']\n",
    "data[[x + '_Growth' for x in YY_cols]] = data[YY_cols]/data[YY_cols].shift(12 if lin_interp else 4) - 1\n",
    "\n",
    "#MISSING COMMODITIES INDEX AND INTEREST RATES INDEX\n",
    "assets = ['SPY', 'US_Corp', 'LIBOR']\n",
    "data[[x + '_Return' for x in assets]] = data[assets]/data[assets].shift(1) - 1\n",
    "\n",
    "#Drop null rows\n",
    "data = data.dropna()\n",
    "\n",
    "#Subtract mean\n",
    "# data = data - data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TNX                   4.119000\n",
       "US_Corp            1541.775909\n",
       "LIBOR                 2.397496\n",
       "CPI                 205.900000\n",
       "BAA                   6.580000\n",
       "UNRATE                5.400000\n",
       "SPY                 125.500000\n",
       "IRX                   1.867000\n",
       "RGDP              15189.222000\n",
       "YC_Slope              1.744000\n",
       "Cred_Spread           2.340000\n",
       "CPI_Growth            0.021816\n",
       "RGDP_Growth           0.025619\n",
       "SPY_Return            0.012632\n",
       "US_Corp_Return        0.005864\n",
       "LIBOR_Return          0.000522\n",
       "dtype: float64"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TNX                  1.650079\n",
       "US_Corp            737.207299\n",
       "LIBOR                2.203278\n",
       "CPI                 31.988520\n",
       "BAA                  1.401699\n",
       "UNRATE               1.632186\n",
       "SPY                 62.236253\n",
       "IRX                  2.062834\n",
       "RGDP              2536.957367\n",
       "YC_Slope             1.077613\n",
       "Cred_Spread          0.779342\n",
       "CPI_Growth           0.005223\n",
       "RGDP_Growth          0.016148\n",
       "SPY_Return           0.041065\n",
       "US_Corp_Return       0.012934\n",
       "LIBOR_Return         0.085830\n",
       "dtype: float64"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.267446586719577, 4.695344110954851)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quick function to get num_sd standard deviations away from the median.\n",
    "#Up pos determines if up is good or bad. Idea being that good scenario is at index 1, bad scenario at index 0.\n",
    "def get_range(data, variable, num_sd, up_pos = True):\n",
    "    i = 1 if up_pos else -1\n",
    "    \n",
    "    v = data[variable]\n",
    "    v = v[(v < v.quantile(.85) ) & (v > v.quantile(.15))]\n",
    "    m = v.mean()\n",
    "    s = v.std()\n",
    "    \n",
    "    return tuple((m - i * num_sd * s, m + i * num_sd * s))\n",
    "\n",
    "get_range(data, 'UNRATE', 1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Scenarios\n",
    "\n",
    "- Define a scenario with a boolean string. Make sure to use spaces between operators (for calculation of mahalanobis distances.\n",
    "- Find empirical mean and covariance matrix of the factors in this scenario.\n",
    "- Encode scenario as a vector, then find mahalanobis distance using scenario vector and empirical mean, covariance.\n",
    "- We then convert scenario Mahalanobis distance into likelihood measure:\n",
    "$$ e^{\\frac{-d}{2}}$$\n",
    "- Rescale probabilities to sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scenario_vars(scenario):\n",
    "    #Get the variables in the scenario. Sort alphabetically for consistent replication\n",
    "    return sorted(list(set([v.split(' ')[0] for v in scenario.split(' & ')])))\n",
    "\n",
    "def get_scenario_vector(scenario):\n",
    "    #Get a vector from a scenario.\n",
    "    vector = {}\n",
    "    for v in scenario.split(' & '):\n",
    "        x = v.split(' ')\n",
    "        cn = x[0]\n",
    "        val = x[2]\n",
    "        vector[cn] = float(val)\n",
    "    vector = pd.DataFrame(vector, index = [0])\n",
    "    colnames = get_scenario_vars(scenario)\n",
    "    return vector[colnames].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGDP_Growth 0.019330980451302004 0.03436341235909478\n",
      "UNRATE 6.267446586719577 4.695344110954851\n",
      "CPI_Growth 0.019079318912049274 0.024393502210745426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Weak': 'RGDP_Growth <= 0.019330980451302004 & UNRATE >= 6.267446586719577 & CPI_Growth <= 0.019079318912049274',\n",
       " 'Strong': 'RGDP_Growth >= 0.03436341235909478 & UNRATE <= 4.695344110954851 & CPI_Growth >= 0.024393502210745426',\n",
       " 'Normal': 'RGDP_Growth >= 0.019330980451302004 & RGDP_Growth <= 0.03436341235909478 & UNRATE <= 6.267446586719577 & UNRATE >= 4.695344110954851 & CPI_Growth >= 0.019079318912049274 & CPI_Growth <= 0.024393502210745426'}"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build the scenarios: Weak is >= 1 sd below median, strong is >= 1 sd above median, normal is in range [m - sd, m + sd].\n",
    "\n",
    "scen_names = ['Weak', 'Strong', 'Normal']\n",
    "# relevant_vars = ['RGDP_Growth', 'UNRATE', 'CPI_Growth', 'YC_Slope', 'Cred_Spread', 'TNX']\n",
    "relevant_vars = ['RGDP_Growth', 'UNRATE', 'CPI_Growth']\n",
    "# relevant_vars = ['RGDP_Growth']\n",
    "\n",
    "scenarios = {sn: '' for sn in scen_names}\n",
    "\n",
    "up_pos = {v:True for v in relevant_vars}\n",
    "up_pos['UNRATE'] = False\n",
    "up_pos['YC_Slope'] = False\n",
    "up_pos['Cred_Spread'] = False\n",
    "\n",
    "first_run = True\n",
    "\n",
    "for var in relevant_vars:\n",
    "    if first_run:\n",
    "        first_run = False\n",
    "    else:\n",
    "        for v in scen_names:\n",
    "            scenarios[v] += ' & '\n",
    "    \n",
    "    low_bound, u_bound = get_range(data, var, 1, up_pos[var])\n",
    "    \n",
    "    l_sign = '<=' if up_pos[var] else '>='\n",
    "    u_sign = '>=' if up_pos[var] else '<='\n",
    "    \n",
    "    scenarios['Weak'] += ('{} {} {}'.format(var, l_sign, low_bound))\n",
    "    scenarios['Normal'] += ('{} {} {} & {} {} {}'.format(var, u_sign, low_bound, var, l_sign, u_bound))\n",
    "    scenarios['Strong'] += ('{} {} {}'.format(var, u_sign, u_bound))\n",
    "    \n",
    "    print(var, low_bound, u_bound)\n",
    "    \n",
    "scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.60599299e-02 8.65029921e-04 8.55517241e+00]\n",
      "[0.025002   0.04399006 4.16666667]\n",
      "[0.02132038 0.0264021  5.092     ]\n",
      "{'Weak': 0.2456491167361094, 'Strong': 0.4487394697597759, 'Normal': 0.7159236143297024}\n",
      "{'Weak': 0.1741806648147183, 'Strong': 0.3181844909921978, 'Normal': 0.5076348441930839}\n"
     ]
    }
   ],
   "source": [
    "likelihoods = {}\n",
    "l_sum = 0.0\n",
    "\n",
    "for scenario_name, scenario in scenarios.items():\n",
    "    #Get relevant variables for scenario defined above\n",
    "    scenario_vars = get_scenario_vars(scenario)\n",
    "\n",
    "    #Get the empirical mean & covariance matrix of scenario vars\n",
    "    v = data[scenario_vars].mean().values\n",
    "    scen_cov = data[scenario_vars].corr()\n",
    "\n",
    "    #Encode the scenario as a vector - take empirical averages after conditioning on scenario\n",
    "    u = data.query(scenario)[scenario_vars].mean().values\n",
    "    print(u)\n",
    "    #u = get_scenario_vector(scenario)\n",
    "\n",
    "    #Mahalanobis distance, converted to likelihood\n",
    "    l = np.exp(-maha(u, v, scen_cov)/2)\n",
    "    likelihoods[scenario_name] = l\n",
    "    l_sum += l\n",
    "\n",
    "probs = {sn: l/l_sum for sn, l in likelihoods.items()}\n",
    "print(likelihoods)\n",
    "print(probs)\n",
    "# print(mahala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weak (29, 3)\n",
      "                SPY_Return  US_Corp_Return  LIBOR_Return\n",
      "SPY_Return        0.001815       -0.000006     -0.000028\n",
      "US_Corp_Return   -0.000006        0.000297     -0.001422\n",
      "LIBOR_Return     -0.000028       -0.001422      0.010969\n",
      "Strong (6, 3)\n",
      "                SPY_Return  US_Corp_Return  LIBOR_Return\n",
      "SPY_Return        0.007265        0.000209      0.000100\n",
      "US_Corp_Return    0.000209        0.000058     -0.000055\n",
      "LIBOR_Return      0.000100       -0.000055      0.000138\n",
      "Normal (25, 3)\n",
      "                SPY_Return  US_Corp_Return  LIBOR_Return\n",
      "SPY_Return        0.001567       -0.000105     -0.000428\n",
      "US_Corp_Return   -0.000105        0.000112     -0.000210\n",
      "LIBOR_Return     -0.000428       -0.000210      0.003164\n",
      "\n",
      " *********** Returns *********** \n",
      "                     Weak    Strong    Normal\n",
      "LIBOR_Return   -0.071986 -0.002049  0.019163\n",
      "SPY_Return      0.009426  0.013629  0.006680\n",
      "US_Corp_Return  0.010003  0.014642  0.002303\n"
     ]
    }
   ],
   "source": [
    "#Get expected historical returns for each scenario.\n",
    "#Likely want historical variances also.\n",
    "returns = pd.DataFrame()\n",
    "covariances = {}\n",
    "returns_cols = [x for x in scen_data.columns if 'Return' in x]\n",
    "\n",
    "\n",
    "for scenario_name, scenario in scenarios.items():\n",
    "    scen_data = data.query(scenario)[returns_cols]\n",
    "    print(scenario_name, scen_data.shape)\n",
    "    m = scen_data.mean().rename('{}'.format(scenario_name))\n",
    "    returns = returns.append(m)\n",
    "    cov_mat = scen_data.cov()\n",
    "    covariances[scenario_name] = cov_mat\n",
    "    print(cov_mat)\n",
    "\n",
    "    \n",
    "# x\n",
    "#Column: Scenario\n",
    "#Row: Expected return\n",
    "print(\"\\n *********** Returns *********** \\n\", returns.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disregard below - not right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "1993-01-01    0.007402\n",
       "1993-04-01    0.008183\n",
       "1993-07-01    0.009043\n",
       "1993-10-01    0.009513\n",
       "1994-01-01    0.010511\n",
       "1994-04-01    0.011619\n",
       "1994-07-01    0.013492\n",
       "1994-10-01    0.015674\n",
       "1995-01-01    0.014360\n",
       "1995-04-01    0.015669\n",
       "1995-07-01    0.015091\n",
       "1995-10-01    0.013657\n",
       "1996-01-01    0.014356\n",
       "1996-04-01    0.014343\n",
       "1996-07-01    0.013654\n",
       "1996-10-01    0.011751\n",
       "1997-01-01    0.012357\n",
       "1997-04-01    0.011172\n",
       "1997-07-01    0.010112\n",
       "1997-10-01    0.009153\n",
       "1998-01-01    0.008706\n",
       "1998-04-01    0.007493\n",
       "1998-07-01    0.008279\n",
       "1998-10-01    0.008277\n",
       "1999-01-01    0.007493\n",
       "1999-04-01    0.007494\n",
       "1999-07-01    0.007491\n",
       "1999-10-01    0.006776\n",
       "2000-01-01    0.006453\n",
       "2000-04-01    0.005832\n",
       "                ...   \n",
       "2012-04-01    0.004721\n",
       "2012-07-01    0.004720\n",
       "2012-10-01    0.005764\n",
       "2013-01-01    0.005219\n",
       "2013-04-01    0.006371\n",
       "2013-07-01    0.007406\n",
       "2013-10-01    0.007785\n",
       "2014-01-01    0.010500\n",
       "2014-04-01    0.012841\n",
       "2014-07-01    0.012840\n",
       "2014-10-01    0.015094\n",
       "2015-01-01    0.015092\n",
       "2015-04-01    0.012990\n",
       "2015-07-01    0.011758\n",
       "2015-10-01    0.010641\n",
       "2016-01-01    0.010118\n",
       "2016-04-01    0.010638\n",
       "2016-07-01    0.009625\n",
       "2016-10-01    0.010118\n",
       "2017-01-01    0.009155\n",
       "2017-04-01    0.007880\n",
       "2017-07-01    0.007494\n",
       "2017-10-01    0.006781\n",
       "2018-01-01    0.006782\n",
       "2018-04-01    0.006135\n",
       "2018-07-01    0.006136\n",
       "2018-10-01    0.005839\n",
       "2019-01-01    0.006450\n",
       "2019-04-01    0.005282\n",
       "2019-07-01    0.005553\n",
       "Name: Prob_1, Length: 107, dtype: float64"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define scenario, use spaces between operators for ease of parsing.\n",
    "scenario = 'RGDP_Growth = 0.01 & UNRATE = 6'\n",
    "\n",
    "#Get relevant variables from the scenario defined above.\n",
    "scenario_vars = [v.split(' ')[0] for v in scenario.split(' & ')]\n",
    "\n",
    "#Filter data on the above scenario - not sure if useful or not...\n",
    "scen_data = data.query(scenario)[scenario_vars]\n",
    "\n",
    "#Get the empirical mean & correlation matrix of scenario vars\n",
    "v = data[scenario_vars].mean().values\n",
    "scen_corr = data[scenario_vars].corr()\n",
    "\n",
    "#Calculate mahalanobis distance, transform to likelihood measure\n",
    "data['L_1'] = data[scenario_vars].apply(lambda x: np.exp(-maha(x ,v, scen_corr)/2), raw = True, axis = 1)\n",
    "\n",
    "#Rescale for likelihood\n",
    "data['Prob_1'] = data['L_1'].dropna()/data['L_1'].sum()\n",
    "# data['Prob_1'] = data['Prob_1'].fillna(0)\n",
    "data.Prob_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_1 = 'RGDP_Growth = 0.01 & UNRATE = 6'\n",
    "scenario_2 = 'RGDP_Growth '\n",
    "\n",
    "# l_1 = np.exp(-maha(np.array([float(v.split(' ')[2]) for v in scenario_1.split(' & ')]), v, scen_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RGDP_Growth', 'UNRATE']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scenario_vars(scenario_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
