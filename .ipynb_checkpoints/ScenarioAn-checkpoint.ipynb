{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis as maha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- Load Data\n",
    "- Calculate yield curve slope and Y/Y changes in relevant columns\n",
    "\n",
    "\n",
    "#### Data Definitions\n",
    "- TNX: US 10y Treasury\n",
    "- US_Corp: ML US Corporate Bond Total Return Index\n",
    "- LIBOR: 3m LIBOR Rate\n",
    "- BAA: Moody's long-term corporate bond yields index\n",
    "- UNRATE: US seasonally-adjusted unemployment rate\n",
    "- SPY: S\\&P 500 Index\n",
    "- IRX: US 3m treasury rate\n",
    "- RGDP: US seasonally-adjusted Real GDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Commodity Index Data\n",
    "data = pd.read_csv('data/data.csv', index_col=0)\n",
    "data.index = pd.to_datetime(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Interpolation Forward fill - for GDP data which is quarterly\n",
    "#Could potentially want to only use quarterly data - set lin_interp = False\n",
    "lin_interp = False\n",
    "\n",
    "if lin_interp:\n",
    "    data['RGDP'] = data['RGDP'].interpolate()\n",
    "else:\n",
    "    data = data.dropna(subset = ['RGDP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yield Curve Slope: 10y yields - 3m yields\n",
    "data['YC_Slope'] =  pd.eval('data.TNX - data.IRX')\n",
    "#Credit Spread: long-term BAA (corp bonds) - 10y treasury rate\n",
    "data['Cred_Spread'] = pd.eval('data.BAA - data.TNX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in Y/Y changes\n",
    "YY_cols = ['CPI', 'RGDP']\n",
    "data[[x + '_Growth' for x in YY_cols]] = data[YY_cols]/data[YY_cols].shift(1) - 1\n",
    "\n",
    "#Drop null rows\n",
    "data = data.dropna()\n",
    "\n",
    "#Subtract mean\n",
    "# data = data - data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Scenarios\n",
    "\n",
    "- Define a scenario with a boolean string. Make sure to use spaces between operators (for calculation of mahalanobis distances.\n",
    "- Filtering on this scenario, we calculate the Mahalanobis distance\n",
    "- We then convert scenario Mahalanobis distance into likelihood measure:\n",
    "$$ e^{\\frac{-d}{2}}$$\n",
    "- Rescale probabilities to sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scenario_vars(scenario):\n",
    "    #Get the variables in the scenario. Sort alphabetically for consistent replication\n",
    "    return sorted(list(set([v.split(' ')[0] for v in scenario.split(' & ')])))\n",
    "\n",
    "def get_scenario_vector(scenario):\n",
    "    #Get a vector from a scenario.\n",
    "    vector = {}\n",
    "    for v in scenario.split(' & '):\n",
    "        x = v.split(' ')\n",
    "        cn = x[0]\n",
    "        val = x[2]\n",
    "        vector[cn] = float(val)\n",
    "    vector = pd.DataFrame(vector, index = [0])\n",
    "    colnames = get_scenario_vars(scenario)\n",
    "    return vector[colnames].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Weak': 0.8996118267005236, 'Normal': 0.24809954216648772, 'Strong': 0.8634019768227041}\n",
      "{'Weak': 0.4473203007819532, 'Normal': 0.12336427615988071, 'Strong': 0.42931542305816617}\n"
     ]
    }
   ],
   "source": [
    "#Define scenarios, use spaces between operators for ease of parsing.\n",
    "scenarios = {'Weak': 'RGDP_Growth = 0.01 & UNRATE = 6', \n",
    "             'Normal': 'UNRATE = 3',\n",
    "             'Strong': 'RGDP_Growth = 0.3'}\n",
    "\n",
    "likelihoods = {}\n",
    "\n",
    "l_sum = 0.0\n",
    "\n",
    "for scenario_name, scenario in scenarios.items():\n",
    "    #Get relevant variables for scenario defined above\n",
    "    scenario_vars = get_scenario_vars(scenario)\n",
    "\n",
    "    #Get the empirical mean & correlation matrix of scenario vars\n",
    "    v = data[scenario_vars].mean().values\n",
    "    scen_corr = data[scenario_vars].corr()\n",
    "\n",
    "    #Encode the scenario as a vector\n",
    "    u = get_scenario_vector(scenario)\n",
    "\n",
    "    #Mahalanobis distance, converted to likelihood\n",
    "    l = np.exp(-maha(u, v, scen_corr)/2)\n",
    "    likelihoods[scenario_name] = l\n",
    "    l_sum += l\n",
    "\n",
    "probs = {sn: l/l_sum for sn, l in likelihoods.items()}\n",
    "print(likelihoods)\n",
    "print(probs)\n",
    "# print(mahala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario = 'RGDP_Growth = 0.01 & UNRATE = 6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "1993-01-01    0.007402\n",
       "1993-04-01    0.008183\n",
       "1993-07-01    0.009043\n",
       "1993-10-01    0.009513\n",
       "1994-01-01    0.010511\n",
       "1994-04-01    0.011619\n",
       "1994-07-01    0.013492\n",
       "1994-10-01    0.015674\n",
       "1995-01-01    0.014360\n",
       "1995-04-01    0.015669\n",
       "1995-07-01    0.015091\n",
       "1995-10-01    0.013657\n",
       "1996-01-01    0.014356\n",
       "1996-04-01    0.014343\n",
       "1996-07-01    0.013654\n",
       "1996-10-01    0.011751\n",
       "1997-01-01    0.012357\n",
       "1997-04-01    0.011172\n",
       "1997-07-01    0.010112\n",
       "1997-10-01    0.009153\n",
       "1998-01-01    0.008706\n",
       "1998-04-01    0.007493\n",
       "1998-07-01    0.008279\n",
       "1998-10-01    0.008277\n",
       "1999-01-01    0.007493\n",
       "1999-04-01    0.007494\n",
       "1999-07-01    0.007491\n",
       "1999-10-01    0.006776\n",
       "2000-01-01    0.006453\n",
       "2000-04-01    0.005832\n",
       "                ...   \n",
       "2012-04-01    0.004721\n",
       "2012-07-01    0.004720\n",
       "2012-10-01    0.005764\n",
       "2013-01-01    0.005219\n",
       "2013-04-01    0.006371\n",
       "2013-07-01    0.007406\n",
       "2013-10-01    0.007785\n",
       "2014-01-01    0.010500\n",
       "2014-04-01    0.012841\n",
       "2014-07-01    0.012840\n",
       "2014-10-01    0.015094\n",
       "2015-01-01    0.015092\n",
       "2015-04-01    0.012990\n",
       "2015-07-01    0.011758\n",
       "2015-10-01    0.010641\n",
       "2016-01-01    0.010118\n",
       "2016-04-01    0.010638\n",
       "2016-07-01    0.009625\n",
       "2016-10-01    0.010118\n",
       "2017-01-01    0.009155\n",
       "2017-04-01    0.007880\n",
       "2017-07-01    0.007494\n",
       "2017-10-01    0.006781\n",
       "2018-01-01    0.006782\n",
       "2018-04-01    0.006135\n",
       "2018-07-01    0.006136\n",
       "2018-10-01    0.005839\n",
       "2019-01-01    0.006450\n",
       "2019-04-01    0.005282\n",
       "2019-07-01    0.005553\n",
       "Name: Prob_1, Length: 107, dtype: float64"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define scenario, use spaces between operators for ease of parsing.\n",
    "scenario = 'RGDP_Growth = 0.01 & UNRATE = 6'\n",
    "\n",
    "#Get relevant variables from the scenario defined above.\n",
    "scenario_vars = [v.split(' ')[0] for v in scenario.split(' & ')]\n",
    "\n",
    "#Filter data on the above scenario - not sure if useful or not...\n",
    "scen_data = data.query(scenario)[scenario_vars]\n",
    "\n",
    "#Get the empirical mean & correlation matrix of scenario vars\n",
    "v = data[scenario_vars].mean().values\n",
    "scen_corr = data[scenario_vars].corr()\n",
    "\n",
    "#Calculate mahalanobis distance, transform to likelihood measure\n",
    "data['L_1'] = data[scenario_vars].apply(lambda x: np.exp(-maha(x ,v, scen_corr)/2), raw = True, axis = 1)\n",
    "\n",
    "#Rescale for likelihood\n",
    "data['Prob_1'] = data['L_1'].dropna()/data['L_1'].sum()\n",
    "# data['Prob_1'] = data['Prob_1'].fillna(0)\n",
    "data.Prob_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_1 = 'RGDP_Growth = 0.01 & UNRATE = 6'\n",
    "scenario_2 = 'RGDP_Growth '\n",
    "\n",
    "# l_1 = np.exp(-maha(np.array([float(v.split(' ')[2]) for v in scenario_1.split(' & ')]), v, scen_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RGDP_Growth', 'UNRATE']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scenario_vars(scenario_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
