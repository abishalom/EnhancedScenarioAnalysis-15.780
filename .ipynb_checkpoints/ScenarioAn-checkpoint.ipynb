{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis as maha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- Load Data\n",
    "- Calculate yield curve slope and Y/Y changes in relevant columns\n",
    "\n",
    "\n",
    "#### Data Definitions\n",
    "- TNX: US 10y Treasury\n",
    "- US_Corp: ML US Corporate Bond Total Return Index\n",
    "- LIBOR: 3m LIBOR Rate\n",
    "- BAA: Moody's long-term corporate bond yields index\n",
    "- UNRATE: US seasonally-adjusted unemployment rate\n",
    "- SPY: S\\&P 500 Index\n",
    "- IRX: US 3m treasury rate\n",
    "- RGDP: US seasonally-adjusted Real GDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Commodity Index Data\n",
    "data = pd.read_csv('data/data.csv', index_col=0)\n",
    "data.index = pd.to_datetime(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Interpolation Forward fill - for GDP data which is quarterly\n",
    "#Could potentially want to only use quarterly data - set lin_interp = False\n",
    "lin_interp = False\n",
    "\n",
    "if lin_interp:\n",
    "    data['RGDP'] = data['RGDP'].interpolate()\n",
    "else:\n",
    "    data = data.dropna(subset = ['RGDP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yield Curve Slope: 10y yields - 3m yields\n",
    "data['YC_Slope'] =  pd.eval('data.TNX - data.IRX')\n",
    "#Credit Spread: long-term BAA (corp bonds) - 10y treasury rate\n",
    "data['Cred_Spread'] = pd.eval('data.BAA - data.TNX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in Y/Y changes\n",
    "YY_cols = ['CPI', 'RGDP']\n",
    "data[[x + '_Growth' for x in YY_cols]] = data[YY_cols]/data[YY_cols].shift(1) - 1\n",
    "\n",
    "#Drop null rows\n",
    "data = data.dropna()\n",
    "\n",
    "#Subtract mean\n",
    "# data = data - data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Scenarios\n",
    "\n",
    "- Define a scenario with a boolean string. Make sure to use spaces between operators (for calculation of mahalanobis distances.\n",
    "- Filtering on this scenario, we calculate the Mahalanobis distance\n",
    "- We then convert scenario Mahalanobis distance into likelihood measure:\n",
    "$$ e^{\\frac{-d}{2}}$$\n",
    "- Rescale probabilities to sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "1993-01-01    0.043120\n",
       "1993-04-01    0.039028\n",
       "1993-07-01    0.035311\n",
       "1993-10-01    0.000000\n",
       "1994-01-01    0.030403\n",
       "1994-04-01    0.000000\n",
       "1994-07-01    0.023672\n",
       "1994-10-01    0.000000\n",
       "1995-01-01    0.000000\n",
       "1995-04-01    0.000000\n",
       "1995-07-01    0.000000\n",
       "1995-10-01    0.000000\n",
       "1996-01-01    0.000000\n",
       "1996-04-01    0.000000\n",
       "1996-07-01    0.000000\n",
       "1996-10-01    0.000000\n",
       "1997-01-01    0.000000\n",
       "1997-04-01    0.000000\n",
       "1997-07-01    0.000000\n",
       "1997-10-01    0.000000\n",
       "1998-01-01    0.000000\n",
       "1998-04-01    0.000000\n",
       "1998-07-01    0.000000\n",
       "1998-10-01    0.000000\n",
       "1999-01-01    0.000000\n",
       "1999-04-01    0.000000\n",
       "1999-07-01    0.000000\n",
       "1999-10-01    0.000000\n",
       "2000-01-01    0.000000\n",
       "2000-04-01    0.000000\n",
       "                ...   \n",
       "2012-04-01    0.054731\n",
       "2012-07-01    0.054742\n",
       "2012-10-01    0.055365\n",
       "2013-01-01    0.060427\n",
       "2013-04-01    0.050097\n",
       "2013-07-01    0.043138\n",
       "2013-10-01    0.041034\n",
       "2014-01-01    0.030377\n",
       "2014-04-01    0.000000\n",
       "2014-07-01    0.000000\n",
       "2014-10-01    0.000000\n",
       "2015-01-01    0.000000\n",
       "2015-04-01    0.000000\n",
       "2015-07-01    0.000000\n",
       "2015-10-01    0.000000\n",
       "2016-01-01    0.000000\n",
       "2016-04-01    0.000000\n",
       "2016-07-01    0.000000\n",
       "2016-10-01    0.000000\n",
       "2017-01-01    0.000000\n",
       "2017-04-01    0.000000\n",
       "2017-07-01    0.000000\n",
       "2017-10-01    0.000000\n",
       "2018-01-01    0.000000\n",
       "2018-04-01    0.000000\n",
       "2018-07-01    0.000000\n",
       "2018-10-01    0.000000\n",
       "2019-01-01    0.000000\n",
       "2019-04-01    0.000000\n",
       "2019-07-01    0.000000\n",
       "Name: Prob_1, Length: 107, dtype: float64"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define scenario, use spaces between operators for ease of parsing.\n",
    "scenario = 'RGDP_Growth < 0.01 & UNRATE >= 6'\n",
    "\n",
    "#Get relevant variables from the scenario defined above.\n",
    "scenario_vars = [v.split(' ')[0] for v in scenario.split(' & ')]\n",
    "\n",
    "#Filter data on the above scenario\n",
    "scen_data = data.query(scenario)[scenario_vars]\n",
    "\n",
    "#Get the empirical mean & correlation matrix of scenario vars during specified scenario\n",
    "v = scen_data.mean().values\n",
    "scen_corr = scen_data.corr()\n",
    "\n",
    "#Calculate mahalanobis distance, transform to likelihood measure\n",
    "data['L_1'] = scen_data.apply(lambda x: np.exp(-maha(x ,v, scen_corr)/2), raw = True, axis = 1)\n",
    "\n",
    "#Rescale for likelihood\n",
    "data['Prob_1'] = data['L_1'].dropna()/data['L_1'].sum()\n",
    "data['Prob_1'] = data['Prob_1'].fillna(0)\n",
    "data.Prob_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
